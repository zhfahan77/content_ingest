# ðŸ“‹ Content Ingest

This is a Python Flask-based scraping application that uses Alembic for database migrations. It also connects to Redis and PostgreSQL databases.

To make it easy to set up and run the application, there is a Docker Compose configuration for development.

## ðŸ› ï¸ Prerequisites

Before you can run this application, ensure you have the following software installed on your system:

- Docker
- Docker Compose
- Python (3.6 or higher)

## ðŸš€ Getting Started

Follow these steps to set up and run the application:

### âž¡ï¸ **Clone this repository to your local machine:**

```shell
git clone git@github.com:zhfahan77/content_ingest.git
```

### ðŸ“‚ Change to the project directory:

```shell
cd content_ingest
```

### ðŸ Create a virtual environment and activate it (optional but recommended):

```shell
python -m venv venv
source venv/bin/activate
```

### ðŸ“¦ Install Python dependencies:

```shell
pip install -r requirements.txt
```

### ðŸ“ Copy .env.sample, create .env file

```shell
cp .env.sample .env
```

### ðŸ Export Python Path
```shel
export PYTHONPATH=$PYTHONPATH:$PWD
```

### ðŸ³ Run Redis and Database using Docker-Compose
```shell
docker-compose up -d
```

### ðŸ“œ Running Migrations

Run Alembic migrations to set up the database schema:

```shell
alembic upgrade head
```

### â–¶ï¸ Run Application (Dev)
```shell
python run.my
```

## ðŸ“Œ OR use Makefile

### âž¡ï¸ **Clone this repository to your local machine:**

```shell
git clone git@github.com:zhfahan77/content_ingest.git
```

### ðŸ“‚ Change to the project directory:

```shell
cd content_ingest
```

### ðŸ Create a virtual environment and activate it (optional but recommended):

```shell
python -m venv venv
source venv/bin/activate
```

### ðŸ”„ Bootsrap Application

```shell
make init
```

---

### â‰ï¸ Help
```shell
make help
```

## âœ¨ Features

- Extract the title and body of a resource
- Extract images from a content item
    - Both body images or meta/link tag image references
- Avoid ingesting the same page with different parameters (that donâ€™t alter the page)
    - Removed the query parameters and anchors from URL
- Save (and index) a content item
    - Saved the scraped data into a PostgreSQL DB
- Cache responses from the server
    - Using redis to cache data on deman
- Extract title and body from PDF
    - Basic scraping from PDF

## ðŸ“‹ APIs

### ðŸš€ Scrape a webpage by a given URL
```shell
curl --location 'http://127.0.0.1:5000/scrape/page' \
--header 'Content-Type: application/json' \
--data '{
    "url": "https://www.theguardian.com/politics/2018/aug/19/brexit-tory-mps-warn-of-entryism-threat-from-leave-eu-supporters",
}'
```

- optionally we can pass
    - "reCache": true --> it would recache the content
    - "reScrape": true --> it would rescrape the url

### ðŸš€ Scrape a PDF by a given URL
```shell
curl --location 'http://127.0.0.1:5000/scrape/pdf' \
--header 'Content-Type: application/json' \
--data '{
    "url": "https://www.lazardassetmanagement.com/docs/product/-sp10-/137/lazardonjapan_2023q3.pdf"
}'
```