# ğŸ“‹ Content Ingest

This is a Python Flask-based scraping application that uses Alembic for database migrations. It also connects to Redis and PostgreSQL databases.

To make it easy to set up and run the application, there is a Docker Compose configuration for development.

## ğŸ› ï¸ Prerequisites

Before you can run this application, ensure you have the following software installed on your system:

- Docker
- Docker Compose
- Python (3.6 or higher)

## ğŸš€ Getting Started

Follow these steps to set up and run the application:

### â¡ï¸ **Clone this repository to your local machine:**

```shell
git clone git@github.com:zhfahan77/content_ingest.git
```

### ğŸ“‚ Change to the project directory:

```shell
cd content_ingest
```

### ğŸ Create a virtual environment and activate it (optional but recommended):

```shell
python -m venv venv
source venv/bin/activate
```

### ğŸ“¦ Install Python dependencies:

```shell
pip install -r requirements.txt
```

### ğŸ“ Copy .env.sample, create .env file

```shell
cp .env.sample .env
```

### ğŸ Export Python Path
```shel
export PYTHONPATH=$PYTHONPATH:$PWD
```

### ğŸ³ Run Redis and Database using Docker-Compose
```shell
docker-compose up -d
```

### ğŸ“œ Running Migrations

Run Alembic migrations to set up the database schema:

```shell
alembic upgrade head
```

### â–¶ï¸ Run Application (Dev)
```shell
python run.my
```

## ğŸ“Œ OR use Makefile

### â¡ï¸ **Clone this repository to your local machine:**

```shell
git clone git@github.com:zhfahan77/content_ingest.git
```

### ğŸ“‚ Change to the project directory:

```shell
cd content_ingest
```

### ğŸ Create a virtual environment and activate it (optional but recommended):

```shell
python -m venv venv
source venv/bin/activate
```

### ğŸ”„ Bootsrap Application

```shell
make init
```

---

### â‰ï¸ Help
```shell
make help
```

## âœ¨ Features

- Extract the title and body of a resource
- Extract images from a content item
    - Both body images or meta/link tag image references
- Avoid ingesting the same page with different parameters (that donâ€™t alter the page)
    - Removed the query parameters and anchors from URL
- Save (and index) a content item
    - Saved the scraped data into a PostgreSQL DB
- Cache responses from the server
    - Using redis to cache data on deman
- Extract title and body from PDF
    - Basic scraping from PDF

## ğŸ“‹ APIs

### ğŸš€ Scrape a webpage/pdf by a given URL

API URL: `http://127.0.0.1:5000/scrape`  
METHOD: `POST`  

### Page
```shell
curl --location 'http://127.0.0.1:5000/scrape' \
--header 'Content-Type: application/json' \
--data '{
    "url": "https://www.theguardian.com/politics/2018/aug/19/brexit-tory-mps-warn-of-entryism-threat-from-leave-eu-supporters",
}'
```

### PDF

```shell
curl --location 'http://127.0.0.1:5000/scrape' \
--header 'Content-Type: application/json' \
--data '{
    "url": "https://www.lazardassetmanagement.com/docs/product/-sp10-/137/lazardonjapan_2023q3.pdf"
}'
```

optionally we can pass  
- `"reCache": true` --> it would recache the content  
- `"reScrape": true` --> it would rescrape the url
